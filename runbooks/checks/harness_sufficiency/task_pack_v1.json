{
  "task_pack_id": "task_pack_v1",
  "version": 1,
  "classes": ["research_pdf", "repo_change", "deploy_flow", "long_form_factual"],
  "tasks": [
    {"task_id": "A01", "task_class": "research_pdf", "prompt_template": "Extract and compare five core claims from two PDFs with evidence pointers.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "grounding->validation", "max_fanout": 3, "max_runtime_hint_min": 20},
    {"task_id": "A02", "task_class": "research_pdf", "prompt_template": "Summarise a methods section and identify unsupported claims with page-level evidence.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "grounding->validation", "max_fanout": 3, "max_runtime_hint_min": 20},
    {"task_id": "A03", "task_class": "research_pdf", "prompt_template": "Build a contradiction table across two papers with explicit evidence spans.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "grounding->validation", "max_fanout": 4, "max_runtime_hint_min": 25},
    {"task_id": "A04", "task_class": "research_pdf", "prompt_template": "Answer a focused factual question from one long PDF and include confidence levels.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "grounding->validation", "max_fanout": 2, "max_runtime_hint_min": 15},
    {"task_id": "A05", "task_class": "research_pdf", "prompt_template": "Create a short claim-evidence map from a PDF appendix.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "grounding->validation", "max_fanout": 2, "max_runtime_hint_min": 15},

    {"task_id": "B01", "task_class": "repo_change", "prompt_template": "Fix a failing unit test with minimal code change and run validations.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "validation", "max_fanout": 2, "max_runtime_hint_min": 20},
    {"task_id": "B02", "task_class": "repo_change", "prompt_template": "Add a boundary check to an existing parser and verify regressions.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "validation", "max_fanout": 2, "max_runtime_hint_min": 20},
    {"task_id": "B03", "task_class": "repo_change", "prompt_template": "Refactor duplicated logic while preserving behaviour and tests.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "validation", "max_fanout": 2, "max_runtime_hint_min": 25},
    {"task_id": "B04", "task_class": "repo_change", "prompt_template": "Patch a contract mismatch and regenerate targeted fixtures.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "validation", "max_fanout": 3, "max_runtime_hint_min": 25},
    {"task_id": "B05", "task_class": "repo_change", "prompt_template": "Add deterministic guard rails for an unstable loop and prove bounded behaviour.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "validation", "max_fanout": 3, "max_runtime_hint_min": 25},

    {"task_id": "C01", "task_class": "deploy_flow", "prompt_template": "Run deploy verification checks for a preview environment and collect evidence.", "required_artefacts": ["skill_result", "validator_result", "experience_packet", "merge_authority_audit"], "expected_gate_path": "deploy_verify->validation", "max_fanout": 2, "max_runtime_hint_min": 25},
    {"task_id": "C02", "task_class": "deploy_flow", "prompt_template": "Validate build and smoke test outputs before release decision.", "required_artefacts": ["skill_result", "validator_result", "experience_packet", "merge_authority_audit"], "expected_gate_path": "deploy_verify->validation", "max_fanout": 2, "max_runtime_hint_min": 20},
    {"task_id": "C03", "task_class": "deploy_flow", "prompt_template": "Simulate rollback decision path with gate evidence and reason codes.", "required_artefacts": ["skill_result", "validator_result", "experience_packet", "merge_authority_audit"], "expected_gate_path": "deploy_verify->validation", "max_fanout": 2, "max_runtime_hint_min": 20},
    {"task_id": "C04", "task_class": "deploy_flow", "prompt_template": "Assess deploy drift from last known good artefacts.", "required_artefacts": ["skill_result", "validator_result", "experience_packet", "merge_authority_audit"], "expected_gate_path": "deploy_verify->validation", "max_fanout": 3, "max_runtime_hint_min": 30},
    {"task_id": "C05", "task_class": "deploy_flow", "prompt_template": "Run post-deploy validation checklist and produce pass/fail rationale.", "required_artefacts": ["skill_result", "validator_result", "experience_packet", "merge_authority_audit"], "expected_gate_path": "deploy_verify->validation", "max_fanout": 2, "max_runtime_hint_min": 20},

    {"task_id": "D01", "task_class": "long_form_factual", "prompt_template": "Produce a factual long-form answer with explicit grounding pointers.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "longform_guard->grounding->validation", "max_fanout": 2, "max_runtime_hint_min": 20},
    {"task_id": "D02", "task_class": "long_form_factual", "prompt_template": "Draft a policy memo with claim-level evidence references.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "longform_guard->grounding->validation", "max_fanout": 3, "max_runtime_hint_min": 25},
    {"task_id": "D03", "task_class": "long_form_factual", "prompt_template": "Answer a comparative factual question across multiple sources.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "longform_guard->grounding->validation", "max_fanout": 3, "max_runtime_hint_min": 25},
    {"task_id": "D04", "task_class": "long_form_factual", "prompt_template": "Generate a concise brief with confidence-calibrated conclusions.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "longform_guard->grounding->validation", "max_fanout": 2, "max_runtime_hint_min": 20},
    {"task_id": "D05", "task_class": "long_form_factual", "prompt_template": "Produce a rebuttal-style factual response with source traceability.", "required_artefacts": ["skill_result", "validator_result", "experience_packet"], "expected_gate_path": "longform_guard->grounding->validation", "max_fanout": 3, "max_runtime_hint_min": 25}
  ]
}
